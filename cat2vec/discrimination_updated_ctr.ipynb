{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cat2vec import Options, Cat2Vec\n",
    "import tensorflow as tf\n",
    "from sample_encoding import *\n",
    "from utility import load_data\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "\n",
    "def generate_discriminant_ctr_batch(data, Y, opts):\n",
    "    data_index = 0\n",
    "    while True:\n",
    "        batch = np.ndarray(shape=(opts.batch_size, opts.sequence_length))\n",
    "        labels = np.ndarray(shape=(opts.batch_size, opts.num_classes))\n",
    "        for i in xrange(opts.batch_size):\n",
    "            target = np.zeros(opts.num_classes)\n",
    "            target[Y[data_index]] = 1.\n",
    "            temp = data[data_index][0:opts.sequence_length]\n",
    "            if len(temp) < opts.sequence_length:\n",
    "                gap = opts.sequence_length - len(temp)\n",
    "                temp = np.array(temp + [0] * gap)\n",
    "            assert len(temp) == opts.sequence_length\n",
    "            batch[i] = temp\n",
    "            labels[i] = target\n",
    "            data_index = (data_index + 1) % len(data)\n",
    "        yield batch, labels\n",
    "        \n",
    "    \n",
    "def generate_discriminant_test_batch(data, Y, opts):\n",
    "    data_index = 0\n",
    "    print('Total testing batch number',len(data)//opts.batch_size)\n",
    "    for _ in range(len(data)//opts.batch_size):\n",
    "        batch = np.ndarray(shape=(opts.batch_size, opts.sequence_length))\n",
    "        labels = np.ndarray(shape=(opts.batch_size, opts.num_classes))\n",
    "        for i in xrange(opts.batch_size):\n",
    "            target = np.zeros(opts.num_classes)\n",
    "            target[Y[data_index]] = 1.\n",
    "            temp = data[data_index][0:opts.sequence_length]\n",
    "            if len(temp) < opts.sequence_length:\n",
    "                gap = opts.sequence_length - len(temp)\n",
    "                temp = np.array(temp + [0] * gap)\n",
    "            assert len(temp) == opts.sequence_length\n",
    "            batch[i] = temp\n",
    "            labels[i] = target\n",
    "            data_index = data_index + 1\n",
    "        yield batch, labels\n",
    "        \n",
    "\n",
    "class DiscriminantCTR(Cat2Vec):\n",
    "\n",
    "    def __init__(self, options, session, cate2id, id2cate, pre_trained_emb=None, trainable=True, pre_trained_path=None):\n",
    "        self.pre_trained_emb = None\n",
    "        self.trainable = trainable\n",
    "        if pre_trained_path is not None:\n",
    "            self.load_pre_trained(pre_trained_path)\n",
    "        Cat2Vec.__init__(self, options, session, cate2id, id2cate)\n",
    "        # self.build_graph()\n",
    "        \n",
    "    def load_pre_trained(self, path):\n",
    "        self.pre_trained_emb = np.array(pd.read_csv(path, sep=',',header=None),dtype=np.float32)\n",
    "        print('pre-trained shape',self.pre_trained_emb.shape)\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"Build the model graph.\"\"\"\n",
    "        opts = self._options\n",
    "        first_indices, second_indices = \\\n",
    "            get_batch_pair_indices(opts.batch_size, opts.sequence_length)\n",
    "        # print(first_indices.shape)\n",
    "        # the following is just for example, base class should not include this\n",
    "        # with self._graph.as_default():\n",
    "        self.train_inputs = tf.placeholder(tf.int32,\n",
    "                                           shape=[opts.batch_size,\n",
    "                                                  opts.sequence_length])\n",
    "        self.train_labels = tf.placeholder(tf.int32, shape=[opts.batch_size,\n",
    "                                                            opts.num_classes])\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        with tf.device('/cpu:0'):\n",
    "            if self.pre_trained_emb is None:\n",
    "                self.embeddings = tf.Variable(tf.random_normal([opts.vocabulary_size,\n",
    "                                  opts.embedding_size],\n",
    "                                 stddev=1.0 / np.sqrt(opts.embedding_size)\n",
    "                                 ))\n",
    "            else:\n",
    "                if self.pre_trained_emb.shape == (opts.vocabulary_size, opts.embedding_size):\n",
    "                    self.embeddings = tf.get_variable(name=\"embeddings\", \n",
    "                                                      shape=[opts.vocabulary_size, opts.embedding_size],\n",
    "                                                      dtype=tf.float32,\n",
    "                                                      initializer=tf.constant_initializer(self.pre_trained_emb), \n",
    "                                                      trainable=self.trainable)\n",
    "                    print('Inited by pre-trained embeddings')\n",
    "                else:\n",
    "                    print('pre_trained_emb shape', self.pre_trained_emb.shape )\n",
    "                    print('vocabulary_size,embedding_size',(opts.vocabulary_size, opts.embedding_size))\n",
    "                    raise Exception('Error', 'pre_trained_emb size mismatch')\n",
    "            embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs)\n",
    "#             l2_loss += tf.nn.l2_loss(embed)\n",
    "            encoded = sample_encoding(embed, opts.interaction_times,\n",
    "                                      opts.batch_size, opts.sequence_length,\n",
    "                                      opts.sequence_length, first_indices,\n",
    "                                      second_indices, opts.gate_type,\n",
    "                                      opts.norm_type)\n",
    "#             encoded = tf.reshape(embed,[opts.batch_size,-1])\n",
    "            encoded = tf.concat(1,[encoded,tf.reshape(embed,[opts.batch_size,-1])])\n",
    "#             encoded = tf.reduce_sum(embed,1)\n",
    "            with tf.name_scope(\"output\"):\n",
    "                encoded_size = encoded.get_shape().as_list()[1]\n",
    "                print(encoded.get_shape().as_list())\n",
    "#                 W, b = weight_bias([encoded_size, opts.embedding_size], [\n",
    "#                                    opts.embedding_size], bias_init=0.)\n",
    "#                 fc1 = tf.matmul(encoded, W) + b\n",
    "#                 W, b = weight_bias([encoded_size//2, encoded_size//4], [\n",
    "#                                    encoded_size//4], bias_init=0.)\n",
    "#                 fc2 = tf.matmul(fc1, W) + b\n",
    "                W, b = weight_bias([encoded_size, opts.num_classes], [\n",
    "                                   opts.num_classes], bias_init=0.)\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "                scores = tf.matmul(encoded, W) + b\n",
    "                self.probs = tf.nn.softmax(scores)\n",
    "                self.predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    scores, tf.to_float(self.train_labels))\n",
    "                self.loss = tf.reduce_mean(losses) + opts.l2_reg_lambda * l2_loss\n",
    "\n",
    "            with tf.name_scope(\"accuracy\"):\n",
    "                correct_predictions = tf.equal(\n",
    "                    self.predictions, tf.argmax(self.train_labels, 1))\n",
    "                self.accuracy = tf.reduce_mean(\n",
    "                    tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            self.loss = tf.clip_by_value(self.loss,-50,50) \n",
    "            optimizer = \\\n",
    "                tf.train.GradientDescentOptimizer(opts.learning_rate)\n",
    "            \n",
    "#             optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_operator = \\\n",
    "                optimizer.minimize(self.loss,\n",
    "                                   gate_gradients=optimizer.GATE_NONE)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        \n",
    "    def eval_auc(self, test_data, test_Y):\n",
    "#         test_batch_generator = generate_discriminant_test_batch(test_data,test_Y, self._options)\n",
    "        opts = self._options\n",
    "        probs = []\n",
    "        y = []\n",
    "        losses = 0.\n",
    "        accuracies = []\n",
    "        batch = np.ndarray(shape=(opts.batch_size, opts.sequence_length))\n",
    "        labels = np.ndarray(shape=(opts.batch_size, opts.num_classes))\n",
    "        data_index = 0\n",
    "        batch_num = len(test_data)//opts.batch_size\n",
    "        print('Total testing batch number', batch_num)\n",
    "        for j in range(len(test_data)//self._options.batch_size):\n",
    "            for i in xrange(opts.batch_size):\n",
    "                target = np.zeros(opts.num_classes)\n",
    "                target[test_Y[data_index]] = 1.\n",
    "                temp = test_data[data_index][0:opts.sequence_length]\n",
    "                if len(temp) < opts.sequence_length:\n",
    "                    gap = opts.sequence_length - len(temp)\n",
    "                    temp = np.array([0] * gap + temp)\n",
    "                assert len(temp) == opts.sequence_length\n",
    "                batch[i] = temp\n",
    "                labels[i] = target\n",
    "                data_index = data_index + 1            \n",
    "            feed_dict = {self.train_inputs: batch,\n",
    "                         self.train_labels: labels}\n",
    "            prob, loss, accuracy = self._session.run([self.probs,\n",
    "                                                   self.loss,\n",
    "                                                   self.accuracy],\n",
    "                                                   feed_dict=feed_dict)\n",
    "            probs.extend(prob)\n",
    "            losses += loss\n",
    "            y.extend(np.argmax(labels, axis=1))\n",
    "        probs = [p[1] for p in probs]\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, probs, pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "#         print('AUC:',auc,'avg log loss:',np.mean(losses),'acc:',np.mean(accuracies))\n",
    "        print('AUC:', auc, 'avg log loss:', losses * 1. / batch_num)\n",
    "            \n",
    "    def train(self, batch_generator, num_steps, test_data,test_Y):\n",
    "        opts = self._options\n",
    "        losses = 0.\n",
    "        acc = 0.\n",
    "        start = time()\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels = batch_generator.next()\n",
    "            feed_dict = {self.train_inputs: batch_inputs,\n",
    "                         self.train_labels: batch_labels}\n",
    "            _, loss, accuracy = self._session.run([self.train_operator,\n",
    "                                                   self.loss,\n",
    "                                                   self.accuracy],\n",
    "                                                  feed_dict=feed_dict)\n",
    "            losses += loss\n",
    "            acc += accuracy\n",
    "            if step % 500 == 0:\n",
    "                t = time()-start\n",
    "                if step > 0:\n",
    "                    losses /= 500\n",
    "                    t  /=500\n",
    "                    acc /= 500\n",
    "                print(\"Average loss at step \", step, \": \",\n",
    "                      losses , ' accuracy: ', acc, 'time', t)\n",
    "                \n",
    "                start = time()\n",
    "                losses = 0.\n",
    "                acc = 0.\n",
    "                \n",
    "            if step % 500 == 0:\n",
    "                print('Eval at step ', step)\n",
    "                self.eval_auc(test_data, test_Y)\n",
    "                print('Eval at Done ', step)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = open('./data/ipinyou/training_not_aligned.csv','r')\n",
    "X = []\n",
    "for line in f.readlines():\n",
    "    X.append([int(x) for x in line.strip().split(',')])\n",
    "# f = open('./data/ipinyou/labels.csv','r')\n",
    "Y = np.array(pd.read_csv('./data/ipinyou/labels.csv',sep=',',header=None))\n",
    "# for line in f.readlines():\n",
    "#     Y.append(int(line.strip()))\n",
    "Y = Y.reshape(len(Y))\n",
    "# X_Y = zip(X,Y)\n",
    "print('X length',len(X),'Y length', len(Y),Y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sum([len(x) for x in X])/len(X)\n",
    "# sum([1 for x in X if len(x)<=32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_num = np.count_nonzero(Y)\n",
    "neg_num = len(Y) - pos_num\n",
    "print('positive samples in training:',pos_num)\n",
    "print('negative samples in training:',neg_num)\n",
    "# def balance_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def balance_data(X,y,ratio=0.5,random_seed=1337):\n",
    "    np.random.seed(1337)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(X)))\n",
    "    X = X[shuffle_indices]\n",
    "    y = y[shuffle_indices]\n",
    "    pos_num = np.count_nonzero(y)\n",
    "    neg_num = int(pos_num / ratio)\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    rst = []\n",
    "    for d in zip(y,X):\n",
    "        if pos_count == pos_num and neg_count == neg_num:\n",
    "            break\n",
    "        if int(d[0]) == 1 and pos_count < pos_num:\n",
    "            rst.append(d)\n",
    "            pos_count += 1\n",
    "        if int(d[0]) == 0 and neg_count < neg_num:\n",
    "            rst.append(d)\n",
    "            neg_count += 1\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(rst)))\n",
    "    y_balanced = np.array([r[0] for r in rst])[shuffle_indices]\n",
    "    X_balanced = np.array([r[1] for r in rst])[shuffle_indices]\n",
    "    if np.count_nonzero(y_balanced) != pos_num:\n",
    "        print('error')\n",
    "    print('re-balanced data positive/negative: ',pos_count,'/',neg_count)\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "X_train_balanced,y_train_balanced =  balance_data(np.array(X_train),np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reverse_dictionary_raw = np.array(pd.read_csv('./data/ipinyou/reverse_dictionary_not_aligned.csv',sep=',',header=None))\n",
    "vocabulary_size = 0\n",
    "reverse_dictionary_raw = np.array(pd.read_csv('./data/ipinyou/reverse_dictionary_not_aligned.csv', sep=',', header=None))\n",
    "reverse_dictionary = {}\n",
    "dictionary = {}\n",
    "for item in reverse_dictionary_raw:\n",
    "    reverse_dictionary[int(item[1])] = item[0]\n",
    "    dictionary[item[0]] = int(item[1])\n",
    "if item[1] > vocabulary_size:\n",
    "    vocabulary_size = item[1]\n",
    "vocabulary_size = len(dictionary.keys())\n",
    "print('vocabulary_size: ',vocabulary_size)\n",
    "id2cate = reverse_dictionary\n",
    "cate2id = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opts = Options()\n",
    "print('Loading data...')\n",
    "# data, id2cate, cate2id, vocabulary_size = load_data(debug=True)\n",
    "opts.sequence_length = 22\n",
    "opts.vocabulary_size = vocabulary_size\n",
    "opts.norm_type = 'l2'\n",
    "opts.gate_type = 'p_norm'\n",
    "opts.l2_reg_lambda = 1e-5\n",
    "opts.batch_size = 32\n",
    "opts.embedding_size = 32\n",
    "opts.interaction_times = 1\n",
    "opts.learning_rate = 0.1\n",
    "batch_generator = generate_discriminant_ctr_batch(X_train_balanced,y_train_balanced,opts)\n",
    "print('Building graph')\n",
    "pre_trained_path = './data/ipinyou/pre_trained_embs_1272384_discri_32_shuffle.csv.csv'\n",
    "# pre_trained_path = None\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    discr_ctr = DiscriminantCTR(opts, session, id2cate, cate2id,\n",
    "                                pre_trained_emb=None, \n",
    "                                trainable=True, \n",
    "                                pre_trained_path=pre_trained_path)\n",
    "    print('Training model')\n",
    "    discr_ctr.train(batch_generator, 10001,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
